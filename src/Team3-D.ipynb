{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ioannis Michalainas** (AEM: 10902) and **Maria Charisi** (AEM: 10727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we begin by importing the necessary libraries. We use **pandas** for loading the datasets and **numpy** for saving the results of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will import some functions from *scikit-learn*, including **MLPClassifier** (Multilayer Perceptron) for the model, **StandardScaler** for scaling the data and **cross_val_scrore** for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare some *constants* like the directory that contains the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 224\n",
    "TRAIN_DATA = '../datasets/datasetTV.csv'\n",
    "TEST_DATA = '../datasets/datasetTest.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and preprocess the **train** and **test** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA, header=None)\n",
    "test = pd.read_csv(TEST_DATA, header=None)\n",
    "\n",
    "feature_columns = [f'feature_{i+1}' for i in range(NUM_FEATURES)]\n",
    "\n",
    "train.columns = feature_columns + ['label']\n",
    "test.columns = feature_columns\n",
    "\n",
    "X_train = train[feature_columns]\n",
    "y_train = train['label']\n",
    "\n",
    "X_test = test[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, our data lies in a high-dimensional space (224 dimensions), so we decided to use **feature selection**. Specifically, we will use a *correlation-based* technique to discard features with low correlation to the label, as these are unlikely to provide meaningful information for the classification task. Using feature selection offers the following benefits:\n",
    "- Reduces noise in the data\n",
    "- Decreases the risk of **overfitting**, because the model focuses on learning the underlying patterns rather than memorizing noise\n",
    "- Improves computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_train.corrwith(y_train)\n",
    "correlation_threshold = 0.1\n",
    "selected_features = correlation_matrix[abs(correlation_matrix) > correlation_threshold].index\n",
    "\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we will **scale** the data, as this is essential for models like **Neural Networks**. Scaling improves the *convergence* of *gradient descent* and prevents features with larger numerical ranges from dominating the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_reduced_scaled = scaler.fit_transform(X_train_reduced)\n",
    "X_test_reduced_scaled = scaler.transform(X_test_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed a *Grid Search* to tune the parameters of the MLPClassifier and found that the best combination for our training set is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes = (200,), learning_rate_init = 0.01, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the accuracy of the model using **5-fold cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network - Cross-validation accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, X_train_reduced_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Neural Network - Cross-validation accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will fit our model on the entire training set and make predictions for the given test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_reduced_scaled, y_train)\n",
    "labelsX = model.predict(X_test_reduced_scaled)\n",
    "\n",
    "np.save('../results/labels3.npy', labelsX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also make sure that *labelsX* can be loaded using *numpy.load()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "Number of samples (N): 6955\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    file_path = \"../results/labels3.npy\"\n",
    "    labels = np.load(file_path)\n",
    "    \n",
    "    # checks if it is an 1D array\n",
    "    if labels.ndim == 1:\n",
    "        print(\"File loaded successfully!\")\n",
    "        print(f\"Number of samples (N): {labels.shape[0]}\")\n",
    "    else:\n",
    "        print(f\"Error: Expected 1D array, but got {labels.ndim}D array with shape {labels.shape}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
