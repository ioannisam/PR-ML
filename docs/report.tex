\documentclass{beamer}

\title{Pattern Recognition and Machine Learning Assignment}
\author{Ioannis Michalainas, Maria Charisi}
\date{December 2024}

\setbeamertemplate{footline}[frame number]
\addtocounter{framenumber}{-1}

\begin{document}

\frame{\titlepage}

\begin{frame}

% PART A%

\frametitle{PART A}
In this part, our study involves the quantification of levels of stress of video game players. Our task in is to implement a Maximum Likelihood Estimator to diversify between two classes $\omega_1$ (stress) and $\omega_2$ (no stress). We need to be able to accurately classify a player based on his play-style as either stressed or not stressed (binary classification).

\end{frame}

\begin{frame}
\frametitle{Data}

To perform our analysis we use the following data 
\begin{itemize}
    \item The Probability Density Function (PDF given $\theta$)$$ p(x \mid \theta) = \frac{\pi}{1 + (x - \theta)^2} $$
    \item Training Sets $D_1$ and $D_2$ for both classes ($\omega_1$, $\omega_2$)
    \item A Discriminant Function $$ g(x) = \log P(x \mid \hat{\theta}_1) - \log P(x \mid \hat{\theta}_2) + \log P(\omega_1) - \log P(\omega_2) $$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{A1}
In this part we want to estimate variables $\theta_1$ and $\theta_2$.
We begin by implementing the log likelihood function:
$$
\log L(\theta \mid D) = \sum_{x \in D} \log p(x \mid \theta)
$$
We now need to find $\theta_1$ and $\theta_2$ that maximize this function. A first approach would be to use the gradient of $log L(\theta \mid D)$ to find the optimal $\theta$ values. However, the gradient of this function does not have a closed-form expression, making it computationally challenging to apply this method directly.
\end{frame}

\begin{frame}
\frametitle{$log L(\theta \mid D)$ maximization}
Instead, we take a simpler approach:
\begin{itemize}
    \item Define a range of candidate $\theta$ values that likely contains the true $\theta$.
    \item Evaluate the log-likelihood function for these candidates and select the $\theta$ that maximizes it.
\end{itemize}
Since the data range spans approximately [-4.5, 4.1], we select a slightly wider candidate range for $\theta$, such as [-5, 5], to ensure it includes the optimal value.
\end{frame}

\begin{frame}
\frametitle{Plot}
$\hat{\theta_1}$ estimation:  2.60
$\hat{\theta_2}$ estimation: -3.16
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/A1.png}
    \caption{Visualizing Optimal $\theta$ Values}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{A2}
Now, we need to use Discriminant Function $$ g(x) = \log P(x \mid \hat{\theta}_1) - \log P(x \mid \hat{\theta}_2) + \log P(\omega_1) - \log P(\omega_2) $$ to classify our data.
\end{frame}

\begin{frame}
\frametitle{Priors Calculation}
We calculate the \textit{a priori probabilities} for each class $\omega_i$. We have a total of 12 samples:
\begin{itemize}
    \item 7 classified in $\omega_1$
    \item 5 classified in $\omega_2$
\end{itemize}
\vspace{0.5cm}
So we calculate $P(\omega_1)=7/12$ and $P(\omega_2)=5/12$ 
\end{frame}


\begin{frame}
\frametitle{Plot}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/A2.png}
    \caption{Discriminant Function Visualization}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Observations}
\begin{itemize}
    \item Most of the $D1$ points are classified correctly, as $g(x) > 0$.
    \item All of the $D2$ points are classified correctly, as $g(x) < 0$.
\end{itemize}
\vspace{1cm}
\textbf{Decision Rule:} The decision boundary is at $g(x) = 0$. For any $x$:
\begin{itemize}
    \item If $g(x) > 0$, classify $x$ as $ω1$ (no stress).
    \item If $g(x) < 0$, classify $x$ as $ω2$ (stress).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
The classification rule leads to some misclassifications. While the decision rule works well for most of the data (11/12, 92\%), there are always some trade-offs in classification accuracy. Achieving perfect classification is sometimes not feasible or desirable.

\begin{itemize}
    \item Attempting to perfectly classify all points might lead to \textbf{overfitting}. A model that fits all training data perfectly may not generalize well to unseen data.
    \item The data may inherently contain some ambiguous or \textbf{overlapping} cases that no model can perfectly classify, especially if the two classes are not linearly separable.
\end{itemize}

\end{frame}

% PART B%

\begin{frame}
\frametitle{Part B}
In this part our task is to implement a new classifier for the previous problem, this time using Bayesian Estimation to estimate parameter $\theta$. 
\end{frame}


\begin{frame}
\frametitle{Data}

To perform our analysis we use the following data:
\begin{itemize}
    \item The Probability Density Functions (PDF)$$ p(\theta) = \frac{1}{10\pi \left( 1 + \left(\frac{\theta}{10}\right)^2 \right)}, \:\:\: p(x \mid \theta) = \frac{\pi}{1 + (x - \theta)^2} $$
    \item Training Samples $D_1$ and $D_2$ for both classes ($\omega_1$, $\omega_2$)
    \item A Discriminant Function $$ h(x) = \log P(x \mid D_1) - \log P(x \mid D_2) + \log P(\omega_1) - \log P(\omega_2) $$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{B1}
In this part, our goal is to calculate the a posteriori probability of $\theta$,
$$
p(\theta \mid D) = \frac{p(D \mid \theta) \cdot p(\theta)}{\int_{-\infty}^{\infty} p(D \mid \theta) \cdot p(\theta) \, d\theta}
$$
\end{frame}

\begin{frame}
\frametitle{$\theta$ Candidates}
\begin{itemize}
    \item In Part A, the dataset values were relatively small ([-4.5, 4.1]). Using a range of [-5, 5] was a practical choice because it encompassed the likely $\theta$ values where the likelihood peaks.
    \item In Part B, the prior distribution has a broader support ($(\theta / 10)$ in the denominator suggests a wider possible range). To ensure the posterior distribution adequately integrates both the likelihood and the prior, we expanded the range to [-10, 10].
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Plot}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/B1.png}
    \caption{$P(\theta \mid D_1)$, $P(\theta \mid D_2)$ and $P(\theta)$}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Observations}
\begin{itemize}
    \item The prior distribution $p(\theta)$ is flat and spread out over the range of $\theta$ values. It shows minimal preference for any specific $\theta$, reflecting the prior belief before observing the data.
    \item The posteriors are much more concentrated than the prior, reflecting how the observed data updates the prior belief and provides more precise estimates of $p(\theta)$.
    \item The location of the peaks in the posteriors ($p(\theta \mid D_1)$ near 2 and $p(\theta \mid D_2)$ near -3) shows the influence of the datasets $D_1$ and $D_2$, respectively.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{B2}
We declare posterior predictive distribution $p(x \mid D)$ like so:
$$
p(x \mid D) = \int p(x \mid \theta) p(\theta \mid D) d\theta
$$

Then we declare the discriminant function $h(x)$:
$$
h(x) = \log P(x \mid D_1) - \log P(x \mid D_2) + \log P(\omega_1) - \log P(\omega_2)
$$
\end{frame}

\begin{frame}
\frametitle{Plot}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/B2.png}
    \caption{Discriminant Function Visualization}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Observations}
\begin{itemize}
    \item All of the $D1$ points are classified correctly, as $h(x) > 0$.
    \item All of the $D2$ points are classified correctly, as $h(x) < 0$.
\end{itemize}
\vspace{1cm}
\textbf{Decision Rule:} The decision boundary is at $h(x) = 0$. For any $x$:
\begin{itemize}
    \item If $h(x) > 0$, classify $x$ as $ω1$ (no stress).
    \item If $h(x) < 0$, classify $x$ as $ω2$ (stress).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Maximum Likelihood Estimation vs Bayesian Estimation}

\textbf{Part A:}
\begin{itemize}
    \item Estimates a single value for $\theta$ ($\hat{\theta}_1$ and $\hat{\theta}_2$) that maximizes the likelihood for each class.
    \item The decision boundary is determined by the log-likelihood and prior probabilities.
\end{itemize}



\textbf{Part B:}
\begin{itemize}
    \item Considers the entire distribution of $\theta$ given the data (the posterior distribution) to make predictions.
    \item The decision boundary is determined by the posterior predictive distribution,  which integrates over all possible values of $\theta$ weighted by their posterior probabilities.
\end{itemize}

We attribute the better performance of BE to the fact that we have prior knowledge about the parameter $\theta$, in means of $p(\theta)$.

\end{frame}

% PART C%

\begin{frame}
\frametitle{Part C}
In this part, our study focuses on the classification of three Iris species: Iris setosa, Iris versicolor, and Iris virginica (i.e. three \textit{classes}).
\begin{itemize}
    \item Using the \textbf{sklearn} library, we analyze a dataset of 150 measurements (50 per species).
    \item Only the first two features (sepal length and width) are used.
    \item A DecisionTreeClassifier is implemented to classify 50\% of randomly selected samples after training the model on the remaining 50\%.
\end{itemize}
   
\end{frame}

\begin{frame}
\frametitle{C1.1}
Our goal is to find the optimal depth with respect to \textit{accuracy}. To do so, we iterate in a range of possible depths (1,10), where the optimal depth should lie. We chose this particular range to avoid overfitting. 
\begin{itemize}
    \item Deep descision trees generally lead to overfitting -they tend to capture overly specific patterns that do not generalize well.
    \item This is especially true in our case, where the dataset is relatively small (150 samples).
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Plot}
We create the \textbf{Decision Tree} that yields the best \textit{accuracy}. As shown in the plot, the optimal \textbf{depth} is 3 with \textbf{accurracy} 0.79.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/C1.1.png}
    \caption{Optimal Depth}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{C1.2}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/C1.2.png}
    \caption{Classification}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{C2.1}
In this task, we extend our classification of the three Iris species using a \textbf{Random Forest} classifier with \textit{100 decision trees}. 
\begin{itemize}
    \item From the training set used previously (set A, 50\% of the dataset), we create 100 training sets, each consisting of $\gamma = 50\%$ of set A, using the \textbf{bootstrap} method.
    \item Each decision tree is trained on its respective bootstrap set with a fixed maximum depth.
    \item The test set from the previous task is reused to evaluate the performance of the Random Forest classifier.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Optimal Depth}
Our goal is yet again to find the optimal depth with respect to \textit{accuracy}. To do so, we iterate in a range of possible depths (1,10), where the optimal depth should lie.
\end{frame}

\begin{frame}
\frametitle{Plot}
We create the \textbf{Random Forest} that yields the best \textit{accuracy}. As shown in the plot, the optimal \textbf{depth} is 2 with \textbf{accurracy} 0.83.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/C2.1.png}
    \caption{Optimal Depth}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{C2.2}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{assets/C2.2.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{C2.3}

We calculate the \textit{accuracy} achieved for different values of $\gamma$ on its respective best depth. Essentially, we keep the \textit{best} result we can get for each $\gamma$, as an example. 

\begin{itemize}
    \item For lower values of $\gamma$ (0.1, 0.2, 0.3), the best accuracy varies between 0.80 and 0.81, with slight improvements as gamma increases. 
    \item For higher values (0.4 and above), the accuracy stabilizes at around 0.83, indicating diminishing returns in performance improvement with increasing gamma. 
    \item The best depth remains practically constant and equal to 2 for almost every $\gamma$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{$\gamma$ Influence}
\begin{itemize}
    \item Generally, small $\gamma$ leads to \textbf{higher bias}, because each bootstrap set contains less information about the entire dataset.
    \item On the other hand, for bigger values of $\gamma$ the diversity among bootstrap sets decreases, which can reduce the ensemble's effectiveness at reducing variance.
\end{itemize}
\vspace{0.5cm}
In conclusion, increasing $\gamma$ up to 0.4 seems to benefit our algorithm, but further increasing it does not have any effect on \textit{accuracy} or the best depth for the Random Forest.
\end{frame}

% PART D%

\begin{frame}
\frametitle{PART D}
This is the introduction slide.
\end{frame}

\begin{frame}
\frametitle{2}

\end{frame}

\begin{frame}
\frametitle{3}

\end{frame}

\begin{frame}
\frametitle{4}

\end{frame}

\begin{frame}
\frametitle{5}

\end{frame}

\begin{frame}
\frametitle{6}

\end{frame}

\begin{frame}
\frametitle{7}

\end{frame}

\begin{frame}
\frametitle{8}

\end{frame}

\begin{frame}
\frametitle{9}

\end{frame}

\begin{frame}
\frametitle{10}

\end{frame}

\begin{frame}
\frametitle{11}

\end{frame}

\begin{frame}
\frametitle{12}

\end{frame}

\begin{frame}
\frametitle{13}

\end{frame}

\begin{frame}
\frametitle{14}

\end{frame}

\begin{frame}
\frametitle{15}

\end{frame}

\begin{frame}
\frametitle{16}

\end{frame}

\begin{frame}
\frametitle{17}

\end{frame}

\begin{frame}
\frametitle{18}

\end{frame}

\begin{frame}
\frametitle{19}

\end{frame}

\begin{frame}
\frametitle{20}

\end{frame}

\end{document}